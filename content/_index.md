---
title: "Home"
---

## About {#about}

I am Daisuke Nohara, a fourth-year undergraduate student at the [Institute of Science Tokyo](https://www.isct.ac.jp/en) (formerly Tokyo Institute of Technology), conducting research in the [Rio Yokota Laboratory](https://www.rio.gsic.titech.ac.jp/).
My research focuses on reinforcement learning for improving reasoning in large language models, with a particular interest in understanding and controlling the reasoning behavior of RL-trained models.

I also enjoy building software systems as a hobby, from low-level Rust engines to self-hosted infrastructure.

## Education {#education}

- **Institute of Science Tokyo** (April 2022 – March 2026, expected)
  - School of Computing, Department of Computer Science

## Research Interests {#research-interests}

My research centers on improving and analyzing reasoning in large language models through reinforcement learning.
Specific topics I am currently exploring include:

- **Reasoning length control in RL-trained LLMs** — Investigating optimal output lengths that balance accuracy and computational cost during inference.
- **Reasoning efficiency** — Reducing the computational cost of chain-of-thought reasoning while preserving answer quality.

## Publications {#publications}

### First-Author

- **On the Optimal Reasoning Length for RL-Trained Language Models**
  Daisuke Nohara, Taishi Nakamura, Rio Yokota. arXiv:2602.09591, 2026.
  [[paper]](https://arxiv.org/abs/2602.09591)

### Co-Authored

- **Optimal Sparsity of Mixture-of-Experts Language Models for Reasoning Tasks**
  Taishi Nakamura, Satoki Ishikawa, Masaki Kawamura, Takumi Okamoto, Daisuke Nohara, Jun Suzuki, Rio Yokota. arXiv:2508.18672, 2025.
  [[paper]](https://arxiv.org/abs/2508.18672)

## Hobbies {#hobbies}

- [Reversi](/hobbies/reversi/)
- [Homelab](/hobbies/homelab/)
